{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4122b0",
   "metadata": {},
   "source": [
    "# COMP4318/5318 Assignment 2: Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f508e",
   "metadata": {},
   "source": [
    "### Group number: ...  , SID1: ... , SID2: ..., SID3: ... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24652d1",
   "metadata": {},
   "source": [
    "This template notebook includes code to load the  dataset and a skeleton for the main sections that should be included in the notebook. Please stick to this struture for your submitted notebook.\n",
    "\n",
    "Please focus on making your code clear, with appropriate variable names and whitespace. Include comments and markdown text to aid the readability of your code where relevant. See the specification and marking criteria in the associated specification to guide you when completing your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1144e1",
   "metadata": {},
   "source": [
    "## Setup and dependencies\n",
    "Please use this section to list and set up all your required libraries/dependencies and your plotting environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d51b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in d:\\study\\5318a2\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# !pip install numpy\n",
    "# !pip install Pillow\n",
    "# !pip install matplotlib\n",
    "# !pip install scikit-image\n",
    "# !pip install scipy\n",
    "# !pip install tensorflow\n",
    "# !pip install scikit-learn\n",
    "# !pip install xgboost\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11ab0507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image,ImageEnhance,ImageFilter,ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.filters import unsharp_mask\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold,cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV,ParameterGrid\n",
    "from sklearn.base import clone\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc35bce",
   "metadata": {},
   "source": [
    "## 1. Data loading, exploration, and preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495a12e-9b9d-48f8-9d2a-abbd01c5a594",
   "metadata": {},
   "source": [
    "Code to load the dataset is provided in the following cell. Please proceed with your data exploration and preprocessing in the remainder of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a991e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset training and test sets as numpy arrays\n",
    "# assuming Assignment2Data folder is present in the same directory \n",
    "# as the notebook\n",
    "X_train = np.load('Assignment2Data/X_train.npy')\n",
    "y_train = np.load('Assignment2Data/y_train.npy')\n",
    "X_test = np.load('Assignment2Data/X_test.npy')\n",
    "y_test = np.load('Assignment2Data/y_test.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c97ca0-15b8-4101-b6b5-b76431c3ca17",
   "metadata": {},
   "source": [
    "### Examples of preprocessed data\n",
    "Please print/display some examples of your preprocessed data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac69507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train[0])\n",
    "img_matrix = X_train[0]\n",
    "img=Image.fromarray(img_matrix.astype('uint8'))\n",
    "img_resized = img.resize((255, 255), resample=Image.BILINEAR)\n",
    "img_resized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf64cc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e90fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgArray=[]\n",
    "for i in range(9):\n",
    "    imgArray.append([])\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    if len(imgArray[y_train[i]])>=10:continue\n",
    "    imgArray[y_train[i]].append(X_train[i])\n",
    "    \n",
    "    \n",
    "strips = [np.concatenate(imgArray[i], axis=1) for i in range(9)]\n",
    "mosaic = np.vstack(strips)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e12e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original sample\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(mosaic.astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce4c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display\n",
    "def displayImage(X,Y):\n",
    "    imgArray=[]\n",
    "    for i in range(9):\n",
    "        imgArray.append([])\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(1000):\n",
    "        if len(imgArray[Y[i]])>=10:continue\n",
    "        imgArray[Y[i]].append(X[i])\n",
    "        \n",
    "        \n",
    "    strips = [np.concatenate(imgArray[i], axis=1) for i in range(9)]\n",
    "    mosaic = np.vstack(strips)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(mosaic)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f5bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_contrast_unsharp(X, contrast_factor=2.0, unsharp_radius=1.0, unsharp_amount=1.5):\n",
    "\n",
    "    alpha = contrast_factor\n",
    "    mean = 127.5  \n",
    "    Xf = X.astype(np.float32)\n",
    "    Xc = (Xf - mean) * alpha + mean\n",
    "    Xc = np.clip(Xc, 0, 255)\n",
    "\n",
    "  \n",
    "    blurred = gaussian_filter(Xc, sigma=(0, unsharp_radius, unsharp_radius, 0))\n",
    "    detail = Xc - blurred\n",
    "    Xs = Xc + unsharp_amount * detail\n",
    "    Xs = np.clip(Xs, 0, 255).astype('uint8')\n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d026c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def autocontrast_equalize(img_array: np.ndarray, cutoff: int = 1) -> np.ndarray:\n",
    " \n",
    "    \n",
    "    img = Image.fromarray(img_array)\n",
    "    img = ImageOps.autocontrast(img, cutoff=cutoff)\n",
    "    img = ImageOps.equalize(img)\n",
    "    return np.array(img, dtype=np.uint8)\n",
    "\n",
    "def batch_autocontrast_equalize(X: np.ndarray, cutoff: int = 1) -> np.ndarray:\n",
    "    return np.stack([autocontrast_equalize(img, cutoff) for img in X], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164dd45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def denoise_median_mode(img_array: np.ndarray, median_size: int = 3, mode_size: int = 3) -> np.ndarray:\n",
    "    img = Image.fromarray(img_array)\n",
    "    img = img.filter(ImageFilter.MedianFilter(size=median_size))\n",
    "    img = img.filter(ImageFilter.ModeFilter(size=mode_size))\n",
    "    return np.array(img, dtype=np.uint8)\n",
    "\n",
    "def batch_denoise_median_mode(X: np.ndarray, median_size: int = 3, mode_size: int = 3) -> np.ndarray:\n",
    "\n",
    "    return np.stack([denoise_median_mode(img, median_size, mode_size) for img in X], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f153c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess result\n",
    "\n",
    "#1.Contrast\n",
    "alpha=2.0\n",
    "mean=127.5\n",
    "\n",
    "X=X_train.astype(np.float32)\n",
    "X_train_contrast=(X-mean)*alpha+mean\n",
    "X_train_contrast=np.clip(X_train_contrast,0,255).astype(np.uint8)\n",
    "\n",
    "X=X_test.astype(np.float32)\n",
    "X_test_contrast=(X-mean)*alpha+mean\n",
    "X_test_contrast=np.clip(X_test_contrast,0,255).astype(np.uint8)\n",
    "\n",
    "#2.sharp&contrast\n",
    "X_train_sharp=batch_contrast_unsharp(X_train)\n",
    "X_test_sharp=batch_contrast_unsharp(X_test)\n",
    "\n",
    "#3.autocontrast & equalize\n",
    "X_train_auto=batch_autocontrast_equalize(X_train)\n",
    "X_test_auto=batch_autocontrast_equalize(X_test)\n",
    "\n",
    "#4. median & mode denoise\n",
    "X_train_denoise=batch_denoise_median_mode(X_train)\n",
    "X_test_denoise=batch_denoise_median_mode(X_test)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c2807",
   "metadata": {},
   "outputs": [],
   "source": [
    "displayImage(X_train,y_train)\n",
    "displayImage(X_train_contrast,y_train)\n",
    "displayImage(X_train_sharp,y_train)\n",
    "displayImage(X_train_auto,y_train)\n",
    "displayImage(X_train_denoise,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81bb2b0",
   "metadata": {},
   "source": [
    "## 2. Algorithm design and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c873320",
   "metadata": {},
   "source": [
    "### Algorithm of choice from first six weeks of course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd14b8b",
   "metadata": {},
   "source": [
    "在前六周的学习中，我们所学习的算法包括，Logistic Regression，Naïve Bayes，Ada Boost， Gradient Boosting，KNN，SVM以及Random Forest。\n",
    "对于一个基于图片的分类任务时，我认为最好的选择是Gradient Boosting\n",
    "Gradient Boosting（如 XGBoost / LightGBM）可以在中等规模的图像特征数据上实现最优的分类效果，具有强大的非线性建模能力、良好的泛化能力，并能有效处理高维度特征，这正好契合 PathMNIST 子集的特征结构（高维、小图像、多类别）。\n",
    "对于Logistic Regression，图片分类问题很难线性化处理。\n",
    "对于Naïve Bayes，前提的假设就不成立\n",
    "对于Decision Tree，单棵树容易过拟合，泛化能力差\n",
    "对于Bagging，虽能缓解过拟合但基模型（如树）弱，效果不如 GBDT 精细\n",
    "对于AdaBoost，其对噪声敏感，图像数据中可能存在模糊类边界\n",
    "对于KNN，高维空间中距离计算不可靠，预测阶段慢，难扩展\n",
    "对于SVM，性能好但对大数据训练慢，尤其是输入维度很高时\n",
    "对于Random Forest， 表现稳定但粗粒度建模，效果通常略逊于 GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c366f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_best_gb_params(X_train, y_train, description=\"\"):\n",
    "\n",
    "    # 展平归一化\n",
    "    X_train_flat = X_train.reshape((X_train.shape[0], -1)) / 255.0\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.5]\n",
    "    }\n",
    "\n",
    "    model = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "\n",
    "    print(f\"{description} - 开始 GB 手动网格搜索，共 {len(ParameterGrid(param_grid))} 组参数...\")\n",
    "    for params in tqdm(ParameterGrid(param_grid)):\n",
    "        clf = clone(model).set_params(**params)\n",
    "        scores = cross_val_score(clf, X_train_flat, y_train, cv=2, scoring='accuracy')\n",
    "        avg_score = scores.mean()\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_params = params\n",
    "\n",
    "    print(f\"{description} - Best param: {best_params}\")\n",
    "    print(f\"{description} - Best score: {best_score:.4f}\")\n",
    "    return best_params\n",
    "    \n",
    "def find_best_xgb_params(X_train, y_train, description=\"\"):\n",
    "    # 1. 数据展平和归一化\n",
    "    X_train_flat = X_train.reshape((X_train.shape[0], -1)) / 255.0\n",
    "\n",
    "    # 2. 定义 XGBoost 模型\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        tree_method='gpu_hist',       # 使用 GPU 加速\n",
    "        predictor='gpu_predictor',    # 使用 GPU\n",
    "        use_label_encoder=False,      # 不使用标签编码\n",
    "        eval_metric='mlogloss',       # 多类对数损失\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "    # 3. 超参数搜索空间，只包括 n_estimators 和 learning_rate\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.5]\n",
    "    }\n",
    "\n",
    "    # 4. 使用 GridSearchCV 来寻找最佳的超参数组合\n",
    "    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, \n",
    "                               scoring='accuracy', n_jobs=1, cv=2, verbose=2)\n",
    "    \n",
    "    # 5. 进行网格搜索（不进行训练）\n",
    "    grid_search.fit(X_train_flat, y_train)\n",
    "\n",
    "    # 输出最佳超参数组合\n",
    "    print(f\"{description} - Best XGBoost Hyperparameters: {grid_search.best_params_}\")\n",
    "    print(f\"{description} - Best cross-validation score for XGBoost: {grid_search.best_score_}\")\n",
    "\n",
    "    # 返回最佳超参数\n",
    "    return grid_search.best_params_\n",
    "    \n",
    "def train_and_evaluate_gb(X_train, y_train, X_test, y_test, description=\"\"):\n",
    "    # 1. 数据展平和归一化\n",
    "    X_train_flat = X_train.reshape((X_train.shape[0], -1)) / 255.0\n",
    "    X_test_flat = X_test.reshape((X_test.shape[0], -1)) / 255.0\n",
    "\n",
    "    # 2. Gradient Boosting 模型\n",
    "    gb_model = GradientBoostingClassifier(n_estimators=50, learning_rate=0.5, random_state=0)\n",
    "    \n",
    "    # 交叉验证\n",
    "    cvKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    cv_scores = cross_val_score(gb_model, X_train_flat, y_train, cv=cvKFold, scoring='accuracy')\n",
    "    print(f\"{description} - GB Cross-validation accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "    # 训练 Gradient Boosting\n",
    "    gb_model.fit(X_train_flat, y_train)\n",
    "\n",
    "    # 测试集评估\n",
    "    y_pred_gb = gb_model.predict(X_test_flat)\n",
    "    gb_acc = accuracy_score(y_test, y_pred_gb)\n",
    "    print(f\"{description} - GB Test Accuracy: {gb_acc:.4f}\")\n",
    "    print(f\"{description} - GB Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_gb))\n",
    "    \n",
    "def train_and_evaluate_xgb(X_train, y_train, X_test, y_test, description=\"\"):\n",
    "    # 1. 数据展平和归一化\n",
    "    X_train_flat = X_train.reshape((X_train.shape[0], -1)) / 255.0\n",
    "    X_test_flat = X_test.reshape((X_test.shape[0], -1)) / 255.0\n",
    "\n",
    "    # 2. XGBoost 模型（使用 GPU）\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        tree_method='gpu_hist',       \n",
    "        predictor='gpu_predictor',    \n",
    "        n_estimators=50,              \n",
    "        learning_rate=0.5,            \n",
    "        max_depth=6,                  \n",
    "        use_label_encoder=False,      \n",
    "        eval_metric='mlogloss',       \n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "    # 3. 交叉验证\n",
    "    cvKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    xgb_scores = cross_val_score(xgb_model, X_train_flat, y_train, cv=cvKFold, scoring='accuracy')\n",
    "    print(f\"{description} - XGBoost Cross-validation accuracy: {xgb_scores.mean():.4f} ± {xgb_scores.std():.4f}\")\n",
    "\n",
    "    # 4. 训练 XGBoost\n",
    "    xgb_model.fit(X_train_flat, y_train)\n",
    "\n",
    "    # 5. 测试集评估\n",
    "    y_pred_xgb = xgb_model.predict(X_test_flat)\n",
    "    xgb_acc = accuracy_score(y_test, y_pred_xgb)\n",
    "    print(f\"{description} - XGBoost Test Accuracy: {xgb_acc:.4f}\")\n",
    "    print(f\"{description} - XGBoost Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_xgb))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f4a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orignal - 开始 GB 手动网格搜索，共 9 组参数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m X_train_test = X_train[:\u001b[32m500\u001b[39m]\n\u001b[32m      2\u001b[39m y_train_test = y_train[:\u001b[32m500\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfind_best_gb_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train_test\u001b[49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morignal\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(find_best_xgb_params(X_train_test,y_train_test,\u001b[33m\"\u001b[39m\u001b[33morignal\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      6\u001b[39m train_and_evaluate_gb(X_train_test,y_train_test,X_test,y_test,\u001b[33m'\u001b[39m\u001b[33mOriginal\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mfind_best_gb_params\u001b[39m\u001b[34m(X_train, y_train, description)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m tqdm(ParameterGrid(param_grid)):\n\u001b[32m     18\u001b[39m     clf = clone(model).set_params(**params)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     scores = \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maccuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     avg_score = scores.mean()\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m avg_score > best_score:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\study\\5318A2\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\study\\5318A2\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:684\u001b[39m, in \u001b[36mcross_val_score\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[39m\n\u001b[32m    681\u001b[39m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[32m    682\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m cv_results = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[33m\"\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\study\\5318A2\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\study\\5318A2\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:411\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[32m    409\u001b[39m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[32m    410\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m results = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\study\\5318A2\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\study\\5318A2\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1985\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1983\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1984\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1985\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1987\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\study\\5318A2\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1913\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1911\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1913\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\study\\5318A2\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     config = {}\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\study\\5318A2\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:866\u001b[39m, in \u001b[36m_fit_and_score\u001b[39m\u001b[34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[39m\n\u001b[32m    864\u001b[39m         estimator.fit(X_train, **fit_params)\n\u001b[32m    865\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m866\u001b[39m         \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[32m    870\u001b[39m     fit_time = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\study\\5318A2\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\study\\5318A2\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:787\u001b[39m, in \u001b[36mBaseGradientBoosting.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, monitor)\u001b[39m\n\u001b[32m    784\u001b[39m     \u001b[38;5;28mself\u001b[39m._resize_state()\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m n_stages = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_stages != \u001b[38;5;28mself\u001b[39m.estimators_.shape[\u001b[32m0\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\study\\5318A2\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:883\u001b[39m, in \u001b[36mBaseGradientBoosting._fit_stages\u001b[39m\u001b[34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[39m\n\u001b[32m    876\u001b[39m         initial_loss = factor * \u001b[38;5;28mself\u001b[39m._loss(\n\u001b[32m    877\u001b[39m             y_true=y_oob_masked,\n\u001b[32m    878\u001b[39m             raw_prediction=raw_predictions[~sample_mask],\n\u001b[32m    879\u001b[39m             sample_weight=sample_weight_oob_masked,\n\u001b[32m    880\u001b[39m         )\n\u001b[32m    882\u001b[39m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m883\u001b[39m raw_predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\study\\5318A2\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:489\u001b[39m, in \u001b[36mBaseGradientBoosting._fit_stage\u001b[39m\u001b[34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[39m\n\u001b[32m    486\u001b[39m     sample_weight = sample_weight * sample_mask.astype(np.float64)\n\u001b[32m    488\u001b[39m X = X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_g_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[32m    494\u001b[39m X_for_tree_update = X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\study\\5318A2\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\study\\5318A2\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1404\u001b[39m, in \u001b[36mDecisionTreeRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m   1374\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, check_input=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1376\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[32m   1377\u001b[39m \n\u001b[32m   1378\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1401\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1402\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1404\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\study\\5318A2\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "X_train_test = X_train[:500]\n",
    "y_train_test = y_train[:500]\n",
    "\n",
    "print(find_best_gb_params(X_train_test,y_train_test,\"orignal\"))\n",
    "print(find_best_xgb_params(X_train_test,y_train_test,\"orignal\"))\n",
    "# train_and_evaluate_gb(X_train_test,y_train_test,X_test,y_test,'Original')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895ee03",
   "metadata": {},
   "source": [
    "### Fully connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c34f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69555204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''  # 强制只用 CPU（上面已设置 TF_DETERMINISTIC_OPS=1）\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "init = keras.initializers.GlorotUniform\n",
    "zeros = keras.initializers.Zeros()\n",
    "\n",
    "data_augment = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\"horizontal\", seed=42),\n",
    "  tf.keras.layers.RandomRotation(0.1, seed=42),\n",
    "  tf.keras.layers.RandomZoom(0.1, seed=42),\n",
    "])\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    data_augment,\n",
    "    keras.layers.Flatten(input_shape=(28,28,3)),\n",
    "    keras.layers.Dense(500, activation='relu',\n",
    "                       kernel_initializer=init(seed=42),\n",
    "                       bias_initializer=zeros),\n",
    "    keras.layers.Dropout(0.3, seed=42),\n",
    "   \n",
    "    keras.layers.Dense(100, activation='relu',\n",
    "                       kernel_initializer=init(seed=43),\n",
    "                       bias_initializer=zeros),\n",
    "    keras.layers.Dropout(0.2, seed=43),\n",
    "    keras.layers.Dense(9, activation='softmax',\n",
    "                       kernel_initializer=init(seed=45),\n",
    "                       bias_initializer=zeros),\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383cc123",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. 归一化到 [0,1]\n",
    "X_train_float = X_train.astype(\"float32\") / 255.0\n",
    "X_test_float  = X_test.astype(\"float32\")  / 255.0\n",
    "\n",
    "# 2. 训练（fit）\n",
    "history = model.fit(\n",
    "    X_train_float, y_train,\n",
    "    shuffle=False,\n",
    "    validation_split=0.1,  \n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                      patience=5,\n",
    "                                      restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3. 查看训练曲线（可选）\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val loss\")\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "# 4. 在测试集上评估\n",
    "test_loss, test_acc = model.evaluate(X_test_float, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# 5. 预测新的样本\n",
    "probs = model.predict(X_test_float[:10])       # 返回 shape=(10,9) 的概率分布\n",
    "preds = probs.argmax(axis=1)             # 取最大概率对应的类别\n",
    "print(\"Predicted:\", preds, \"True:\", y_test[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff94f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_float = X_train.astype(\"float32\") / 255.0\n",
    "X_test_float  = X_test.astype(\"float32\")  / 255.0\n",
    "result=[]\n",
    "for  i in range(10):\n",
    "    for  j in range(10):\n",
    "        #model define\n",
    "        model = keras.models.Sequential([\n",
    "            data_augment,\n",
    "            keras.layers.Flatten(input_shape=(28,28,3)),\n",
    "            keras.layers.Dense((i+1)*100, activation='relu',\n",
    "                            kernel_initializer=init(seed=42),\n",
    "                            bias_initializer=zeros),\n",
    "            keras.layers.Dropout(0.3, seed=42),\n",
    "        \n",
    "            keras.layers.Dense((j+1)*100, activation='relu',\n",
    "                            kernel_initializer=init(seed=43),\n",
    "                            bias_initializer=zeros),\n",
    "            keras.layers.Dropout(0.2, seed=43),\n",
    "            keras.layers.Dense(9, activation='softmax',\n",
    "                            kernel_initializer=init(seed=45),\n",
    "                            bias_initializer=zeros),\n",
    "        ])\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train_float, y_train,\n",
    "            shuffle=False,\n",
    "            validation_split=0.1,  \n",
    "            epochs=20,\n",
    "            batch_size=64,\n",
    "            callbacks=[\n",
    "                keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                            patience=5,\n",
    "                                            restore_best_weights=True)\n",
    "            ]\n",
    "        )\n",
    "        test_loss, test_acc = model.evaluate(X_test_float, y_test, verbose=0)\n",
    "        print(f\"{(i+1)*100} {(j+1)*100} Test accuracy: {test_acc:.4f}\")\n",
    "        result.append(f\"{(i+1)*100} {(j+1)*100} Test accuracy: {test_acc:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3015f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "objectResult=[]\n",
    "for s in result:\n",
    "    split=s.split(\" \")\n",
    "\n",
    "    objectResult.append({\"firstLayer\":split[0],\"secondLayer\":split[1],\"accuracy\":split[-1]})\n",
    "for r in objectResult:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffe3e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# 原始数据\n",
    "data = objectResult\n",
    "\n",
    "# 转换为 DataFrame 并类型转换\n",
    "df = pd.DataFrame(data)\n",
    "df['firstLayer'] = df['firstLayer'].astype(int)\n",
    "df['secondLayer'] = df['secondLayer'].astype(int)\n",
    "df['accuracy'] = df['accuracy'].astype(float)\n",
    "\n",
    "# 生成 3D 三角面片曲面图\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_trisurf(\n",
    "    df['firstLayer'],\n",
    "    df['secondLayer'],\n",
    "    df['accuracy'],\n",
    "    linewidth=0.2\n",
    ")\n",
    "\n",
    "# 设置坐标轴标签\n",
    "ax.set_xlabel('First Layer Units')\n",
    "ax.set_ylabel('Second Layer Units')\n",
    "ax.set_zlabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595926f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_accuracy=sorted(objectResult,key=lambda x:x['accuracy'],reverse=True)\n",
    "print(sorted_by_accuracy[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d620a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. 归一化到 [0,1]\n",
    "X_train_denoise_float = X_train_denoise.astype(\"float32\") / 255.0\n",
    "X_test_denoise_float  = X_test_denoise.astype(\"float32\")  / 255.0\n",
    "\n",
    "# 2. 训练（fit）\n",
    "history = model.fit(\n",
    "    X_train_denoise_float, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                      patience=5,\n",
    "                                      restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3. 查看训练曲线（可选）\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val loss\")\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "# 4. 在测试集上评估\n",
    "test_loss, test_acc = model.evaluate(X_test_denoise_float, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# 5. 预测新的样本\n",
    "probs = model.predict(X_test_denoise_float[:10])       # 返回 shape=(10,9) 的概率分布\n",
    "preds = probs.argmax(axis=1)             # 取最大概率对应的类别\n",
    "print(\"Predicted:\", preds, \"True:\", y_test[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73311603",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. 归一化到 [0,1]\n",
    "X_train_auto_float = X_train_auto.astype(\"float32\") / 255.0\n",
    "X_test_auto_float  = X_test_auto.astype(\"float32\")  / 255.0\n",
    "\n",
    "# 2. 训练（fit）\n",
    "history = model.fit(\n",
    "    X_train_auto_float, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                      patience=5,\n",
    "                                      restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3. 查看训练曲线（可选）\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val loss\")\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "# 4. 在测试集上评估\n",
    "test_loss, test_acc = model.evaluate(X_test_auto_float, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# 5. 预测新的样本\n",
    "probs = model.predict(X_test_auto_float[:10])       # 返回 shape=(10,9) 的概率分布\n",
    "preds = probs.argmax(axis=1)             # 取最大概率对应的类别\n",
    "print(\"Predicted:\", preds, \"True:\", y_test[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aa570f",
   "metadata": {},
   "source": [
    "### Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dbe18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_cnn(X_train, y_train, X_test, y_test, description=\"\"):\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    \n",
    "    # 1. 归一化\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "    # 2. One-hot 编码\n",
    "    y_train_cat = to_categorical(y_train, num_classes)\n",
    "    y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "    # 3. 模型定义\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 3)),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # 4. 模型训练\n",
    "    model.fit(X_train, y_train_cat, epochs=10, batch_size=128, validation_split=0.1, verbose=0)\n",
    "\n",
    "    # 5. 测试评估\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    acc = accuracy_score(y_test, y_pred_labels)\n",
    "    print(f\"{description} - Test Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred_labels))\n",
    "    \n",
    "train_and_evaluate_cnn(X_train, y_train, X_test, y_test, \"original\")\n",
    "train_and_evaluate_cnn(X_train_contrast, y_train, X_test_contrast, y_test, \"contrast\")\n",
    "train_and_evaluate_cnn(X_train_sharp, y_train, X_test_sharp, y_test, \"sharp\")\n",
    "train_and_evaluate_cnn(X_train_auto, y_train, X_test_auto, y_test, \"auto\")\n",
    "train_and_evaluate_cnn(X_train_denoise, y_train, X_test_denoise, y_test, \"denoise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fe14c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_dataset(X: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    N = X.shape[0]\n",
    "    # 1. 扁平化\n",
    "    X_flat = X.reshape(N, -1)           # 正确的 reshape 调用\n",
    "    # 2. 标准化（零均值单位方差）\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_flat)\n",
    "    return X_scaled\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_original_pre=preprocess_dataset(X_train)\n",
    "X_test_original_pre=preprocess_dataset(X_test)\n",
    "\n",
    "X_train_contrast_pre=preprocess_dataset(X_train_contrast)\n",
    "X_test_contrast_pre=preprocess_dataset(X_test_contrast)\n",
    "\n",
    "X_train_sharp_pre=preprocess_dataset(X_train_sharp)\n",
    "X_test_sharp_pre=preprocess_dataset(X_test_sharp)\n",
    "\n",
    "X_train_auto_pre=preprocess_dataset(X_train_auto)\n",
    "X_test_auto_pre=preprocess_dataset(X_test_auto)\n",
    "\n",
    "X_train_denoise_pre=preprocess_dataset(X_train_denoise)\n",
    "X_test_denoise_pre=preprocess_dataset(X_test_denoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2632e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# 定义 MLP\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 128,128),  # 两层：128 + 64 个神经元\n",
    "    activation='relu',             # 激活函数\n",
    "    solver='adam',                 # 优化器\n",
    "    alpha=1e-4,                    # L2 正则化系数\n",
    "    learning_rate_init=1e-3,       # 初始学习率\n",
    "    max_iter=200,                  # 最大迭代次数（Epoch）\n",
    "    random_state=42,\n",
    "    early_stopping=True,           # 如果验证集不再提升则提前停止\n",
    "    validation_fraction=0.1        # 用 10% 验证集做 early stopping\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebeec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "clf.fit(X_train_original_pre, y_train)  \n",
    "# 评估\n",
    "print(\"Train accuracy:\", clf.score(X_train_original_pre, y_train))\n",
    "print(\"Val   accuracy:\", clf.score(X_test_original_pre, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ffceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "clf.fit(X_train_contrast_pre, y_train)  \n",
    "# 评估\n",
    "print(\"Train accuracy:\", clf.score(X_train_contrast_pre, y_train))\n",
    "print(\"Val   accuracy:\", clf.score(X_test_contrast_pre, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "clf.fit(X_train_sharp_pre, y_train)  \n",
    "# 评估\n",
    "print(\"Train accuracy:\", clf.score(X_train_sharp_pre, y_train))\n",
    "print(\"Val   accuracy:\", clf.score(X_test_sharp_pre, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe407ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "clf.fit(X_train_denoise_pre, y_train)  \n",
    "# 评估\n",
    "print(\"Train accuracy:\", clf.score(X_train_denoise_pre, y_train))\n",
    "print(\"Val   accuracy:\", clf.score(X_test_denoise_pre, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f07edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "clf.fit(X_train_auto_pre, y_train)  \n",
    "# 评估\n",
    "print(\"Train accuracy:\", clf.score(X_train_auto_pre, y_train))\n",
    "print(\"Val   accuracy:\", clf.score(X_test_auto_pre, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4911b1",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71085600",
   "metadata": {},
   "source": [
    "### Algorithm of choice from first six weeks of course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d2d615",
   "metadata": {},
   "source": [
    "### Fully connected neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c76117",
   "metadata": {},
   "source": [
    "### Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eead5c82",
   "metadata": {},
   "source": [
    "## 4. Final models\n",
    "In this section, please ensure to include cells to train each model with its best hyperparmater combination independently of the hyperparameter tuning cells, i.e. don't rely on the hyperparameter tuning cells having been run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e56a6c",
   "metadata": {},
   "source": [
    "### Algorithm of choice from first six weeks of course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe676a86",
   "metadata": {},
   "source": [
    "### Fully connected neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c4ab78",
   "metadata": {},
   "source": [
    "### Convolutional neural network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
